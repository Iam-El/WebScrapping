{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# _Question :This is a project to scrape data from the web and store the results in a text file._\n",
    "\n",
    "_1. This code has been developed to scrape the content from the website https://money.cnn.com/data/hotstocks/_\n",
    "\n",
    "_2. We have most_activers,losers,gainers from money website .i have created a separate functions for these 3 and \n",
    "and fetched the data for each ticker symbol from https://finance.yahoo.com/quote/ and put them in a list_\n",
    "\n",
    "_3. Dumped the whole list into CSV(using pandas module) which has stock information on one particular ticker._\n",
    "\n",
    "# _Steps:_\n",
    "\n",
    "_1. Run the program. it takes some time to fetch the content from the money website and put it in CSV._\n",
    "\n",
    "_2. Wait untill the program prompts you to enter some input_\n",
    "\n",
    "_3. Go to jupyter notebook directory (the same directory where our program is) stocks.csv file will be created._\n",
    "\n",
    "_4. File will have a information on most_activers, gainers , losers in a expected format provided by the professor._\n",
    "\n",
    "_5. Eg:-Most Actives,GE,General Electric Co,11.28,11.29,\"22,739,254\",98.427B_\n",
    "\n",
    "_6. When program asks for user input provide the ticker symbol you want the fetch the information on_\n",
    "\n",
    "_7. If you provide the ticker symbol not in CSV then program says 'Entered User ticker doesnt exist!!!' . rerun the       program_\n",
    "\n",
    "_8. When you provide the correct ticker symbol from the CSV it fetches the required stock information from CSV._\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a program to scrape data from the https://money.cnn.com/data/hotstocks/ for a class project.\n",
      "Which stock are you interested in:\n",
      "\n",
      "Most Actives:\n",
      "\n",
      "GE General Electric Co\n",
      "BAC Bank of America Corp\n",
      "T AT&T Inc\n",
      "AMD Advanced Micro Devices Inc\n",
      "F Ford Motor Co\n",
      "MSFT Microsoft Corp\n",
      "AAPL Apple Inc\n",
      "BMY Bristol-Myers Squibb Co\n",
      "FCX Freeport-McMoRan Inc\n",
      "HPQ HP Inc\n",
      "\n",
      "\n",
      "Gainers:\n",
      "\n",
      "NRG NRG Energy Inc\n",
      "HPQ HP Inc\n",
      "CF CF Industries Holdings Inc\n",
      "DXC DXC Technology Co\n",
      "CME CME Group Inc\n",
      "CRM Salesforce.Com Inc\n",
      "DRI Darden Restaurants Inc\n",
      "ICE Intercontinental Exchange Inc\n",
      "VRTX Vertex Pharmaceuticals Inc\n",
      "ATVI Activision Blizzard Inc\n",
      "\n",
      "\n",
      "Losers:\n",
      "\n",
      "APA Apache Corp\n",
      "FTI TechnipFMC PLC\n",
      "DVN Devon Energy Corp\n",
      "KSS Kohls Corp\n",
      "HP Helmerich and Payne Inc\n",
      "NBL Noble Energy Inc\n",
      "URI United Rentals Inc\n",
      "HBI HanesBrands Inc\n",
      "LYB LyondellBasell Industries NV\n",
      "EOG EOG Resources Inc\n",
      "\n",
      "\n",
      "User inputs:HBI\n",
      "The data for HBIHanesBrands Inc is the following:\n",
      "\n",
      "HBI HanesBrands Inc\n",
      "OPEN: 15.44\n",
      "PREV CLOSE: 15.40\n",
      "VOLUME: 1,908,749\n",
      "MARKET CAP: 5.451B\n"
     ]
    }
   ],
   "source": [
    "import requests                # Include HTTP Requests module\n",
    "from urllib.request import urlopen #import urlopen\n",
    "import time # import time module\n",
    "import csv    #import csv module\n",
    "import pandas as pd      #import pandas module\n",
    "from bs4 import BeautifulSoup  # Include BS web scraping module\n",
    "\n",
    "\n",
    "# initialize lists\n",
    "\n",
    "header=[]\n",
    "tddata=[]\n",
    "csvrow1=[]\n",
    "csvrow2=[]\n",
    "csvrow3=[]\n",
    "display1=[]\n",
    "display2=[]\n",
    "display3=[]\n",
    "stockName=[]\n",
    "stockName1=[]\n",
    "count=0\n",
    "\n",
    "\n",
    "# function to fetch the stock information from yahoo website using ticker symbol \n",
    "def web_scrap(a, company,value):\n",
    "    url='https://finance.yahoo.com/quote/'+a # yahoo finance url\n",
    "    stockName.clear()  # clear the list\n",
    "    stockName1.clear() # clear the list\n",
    "    html = urlopen(url) # open url\n",
    "    soup = BeautifulSoup(html, \"lxml\") # use beautifulsoup\n",
    "    div=soup.find('div',{'id':'quote-summary'}) # web scrape the content based on div\n",
    "    div1=div.find('div',{'class':'D(ib) W(1/2) Bxz(bb) Pend(12px) Va(t) ie-7_D(i) smartphone_D(b) smartphone_W(100%) smartphone_Pend(0px) smartphone_BdY smartphone_Bdc($seperatorColor)'}) # further web scrape the content based on inner div\n",
    "    div2=div.find('div',{'class':'D(ib) W(1/2) Bxz(bb) Pstart(12px) Va(t) ie-7_D(i) ie-7_Pos(a) smartphone_D(b) smartphone_W(100%) smartphone_Pstart(0px) smartphone_BdB smartphone_Bdc($seperatorColor)'}) # further web scrape the content based on inner div\n",
    "    rows=div1.find('table') # web scrape the content based on table\n",
    "    rows1=div2.find('table') # web scrape the content based on table\n",
    "    for data in rows:\n",
    "        for cell in data.findAll(['tr']):  # web scrape the content based on tr\n",
    "            for cell1 in cell.findAll(['td']): # web scrape the content based on td\n",
    "                stockName.append(cell1.get_text()) \n",
    "    for data1 in rows1:\n",
    "        for secondcell in data1.findAll(['tr']):  # web scrape the content based on tr\n",
    "            for secondcell1 in secondcell.findAll(['td']): # web scrape the content based on td\n",
    "                stockName1.append(secondcell1.get_text())\n",
    "    return [value,a,company,stockName[3],stockName[1],stockName[13],stockName1[1]]\n",
    "\n",
    "\n",
    "# function to fetch the most archives from the money cnn\n",
    "def most_activers():\n",
    "    value='Most Actives'\n",
    "    html = urlopen('https://money.cnn.com/data/hotstocks/') #open url\n",
    "    soup = BeautifulSoup(html, \"lxml\") # use beautiful soup\n",
    "    table=soup.findAll('table',{'class':'wsod_dataTable wsod_dataTableBigAlt'})[0] # web scrape the content based on 0th table\n",
    "    rows=table.findAll('tr') # web scrape the content based on tr\n",
    "    for data in rows:\n",
    "        for cell in data.findAll(['td'])[0:1]: # web scrape the content based on td\n",
    "            company=cell.get_text()\n",
    "            display1.append(company)\n",
    "            newcompany=company.split(' ', 1)[1]\n",
    "            ticker=company.split() # extract only the ticker\n",
    "            csvrow1.append(web_scrap(ticker[0], newcompany,value)) # function call web_scrap\n",
    "    df1=pd.DataFrame(csvrow1)\n",
    "    df1.to_csv('stocks.csv' ,header=False, index=False) # content is appended to csv file \n",
    "\n",
    "        \n",
    "\n",
    "# function to fetch the gainers from the money cnn        \n",
    "def  Gainers():\n",
    "    value='Gainers'\n",
    "    html = urlopen('https://money.cnn.com/data/hotstocks/') #open url\n",
    "    soup = BeautifulSoup(html, \"lxml\") # use beautiful soup\n",
    "    table=soup.findAll('table',{'class':'wsod_dataTable wsod_dataTableBigAlt'})[1] # web scrape the content based on 1st table\n",
    "    rows=table.findAll('tr') # web scrape the content based on tr\n",
    "    for data in rows:\n",
    "        for cell in data.findAll(['td'])[0:1]:  # web scrape the content based on td\n",
    "            company=cell.get_text()\n",
    "            display2.append(company)\n",
    "            newcompany=company.split(' ', 1)[1]\n",
    "            ticker=company.split() # extract only the ticker\n",
    "            csvrow2.append(web_scrap(ticker[0], newcompany,value)) # function call web_scrap\n",
    "    df2=pd.DataFrame(csvrow2)\n",
    "    df2.to_csv('stocks.csv', mode='a' , header=False, index=False) # content is appended to csv file \n",
    "\n",
    "\n",
    "# function to fetch the losers from the money cnn        \n",
    "def Losers():\n",
    "    value='Losers'\n",
    "    html = urlopen('https://money.cnn.com/data/hotstocks/') #open url\n",
    "    soup = BeautifulSoup(html, \"lxml\") # use beautiful soup\n",
    "    table=soup.findAll('table',{'class':'wsod_dataTable wsod_dataTableBigAlt'})[2] # web scrape the content based on 2nd table\n",
    "    rows=table.findAll('tr') # web scrape the content based on tr\n",
    "    for data in rows:\n",
    "        for cell in data.findAll(['td'])[0:1]: # web scrape the content based on td\n",
    "            company=cell.get_text()\n",
    "            display3.append(company)\n",
    "            newcompany=company.split(' ', 1)[1]\n",
    "            ticker=company.split() # extract only the ticker\n",
    "            csvrow3.append(web_scrap(ticker[0], newcompany,value)) # function call web_scrap\n",
    "    df2=pd.DataFrame(csvrow3)\n",
    "    df2.to_csv('stocks.csv', mode='a' , header=False, index=False) # content is appended to csv file \n",
    "\n",
    "        \n",
    "print(\"This is a program to scrape data from the https://money.cnn.com/data/hotstocks/ for a class project.\")\n",
    "print('Which stock are you interested in:\\n')\n",
    "most_activers() # function call\n",
    "Gainers() # function call\n",
    "Losers() # function call\n",
    "print('Most Actives:\\n')\n",
    "for i in display1:\n",
    "    print(i) # display most_activers\n",
    "print('\\n')\n",
    "print('Gainers:\\n')\n",
    "for i in display2: # display gainers\n",
    "    print(i)\n",
    "print('\\n')\n",
    "print('Losers:\\n')\n",
    "for i in display3:  # display losers\n",
    "    print(i)\n",
    "print('\\n')\n",
    "\n",
    "a=input('User inputs:')\n",
    "with open('stocks.csv', 'rt') as csvFile: # read csv file\n",
    "    reader = csv.reader(csvFile)\n",
    "    for row in reader:\n",
    "        if a==row[1]: # print the stck values\n",
    "            print('The data for ' + row[1]  + row[2] + ' is the following:\\n')\n",
    "            count=count+1\n",
    "            print(row[1],row[2])\n",
    "            print('OPEN:',row[3])\n",
    "            print('PREV CLOSE:',row[4])\n",
    "            print('VOLUME:',row[5])\n",
    "            print('MARKET CAP:',row[6])\n",
    "            break\n",
    "if count==0:  # if the count is zero the entered input doesnt exist\n",
    "    print('Entered User ticker doesnt exist!!!')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
